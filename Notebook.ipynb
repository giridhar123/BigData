{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project\n",
    "Buongiorno a tutti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inizializziamo la SparkSession e importiamo le librerie\n",
    "import findspark\n",
    "\n",
    "location = findspark.find()\n",
    "findspark.init(location)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark ML example on Credit Card data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lettura del file da hdfs con spark e conteggio delle colonne con valori nulli/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = spark.read \\\n",
    "    .csv('hdfs://localhost:9099/creditCardProject/creditcard.csv', header = 'True', inferSchema='True')\n",
    "\n",
    "nullValues = dataSet.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataSet.columns])\n",
    "nullValues.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertiamo in Pandas per una gestione più semplice per poter fare i grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = dataSet.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafico X: Amount --> Y: Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Amount']\n",
    "Y = df['Class']\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.scatter(X, Y, marker = \".\", color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile dedurre che il dataset è sbilanciato e che le transazioni con class = 1 (frodi) hanno un valore di Amount relativamente basso (< 5000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafico: X: Time -- Y:Amount (Rosso frode - Blu non frode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico non frode\n",
    "xlim = df['Time'].max() * 1.2\n",
    "ylim = df['Amount'].max() * 1.2\n",
    "\n",
    "X = np.where(df['Class']==0, df['Time'], None)\n",
    "Y = np.where(df['Class']==0, df['Amount'], None)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.scatter(X, Y, marker = \".\", color = \"blue\")\n",
    "plt.xlim(0, xlim)\n",
    "plt.ylim(0, ylim)\n",
    "plt.show()\n",
    "\n",
    "#Grafico frode\n",
    "X = np.where(df['Class']==1, df['Time'], None)\n",
    "Y = np.where(df['Class']==1, df['Amount'], None)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xlim(0, xlim)\n",
    "plt.ylim(0, ylim)\n",
    "plt.scatter(X, Y, marker = \".\", color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondo me (Davide) è possibile fare discorsi sulla colonna Time, eliminabile a prescindere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# shuffle before creating subsamples\n",
    "\n",
    "df_fraud = df.loc[df.Class == 1]\n",
    "df_non_fraud = df.loc[df.Class == 0][1000:1492]\n",
    "\n",
    "dfFifty = pd.concat([df_fraud, df_non_fraud])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = dfFifty.corr()\n",
    "corr = pd.DataFrame.abs(corr)\n",
    "fig = sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
    "fig.set_title('Correlation Matrix', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "soglia = 0.05\n",
    "print(corr[\"Class\"] < soglia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo matrice di covarianza per capire quali attributi è possibile eliminare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soglia = 0.05\n",
    "#print(corr[\"Class\"] < soglia)\n",
    "#droppiamo V13 e V23 perchè minori della soglia\n",
    "dfFinal = df.drop([\"V13\", \"V23\"], axis = 1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "minMaxScaler = preprocessing.MinMaxScaler()\n",
    "scaledTime = minMaxScaler.fit_transform(dfFinal[['Time']])\n",
    "scaledAmount = minMaxScaler.fit_transform(dfFinal[['Amount']])\n",
    "\n",
    "dfFinal['ScaledTime'] = scaledTime\n",
    "dfFinal['ScaledAmount'] = scaledAmount\n",
    "\n",
    "dfFinal = dfFinal.drop(['Time', 'Amount'], axis = 1)\n",
    "\n",
    "X = dfFinal.iloc[:, dfFinal.columns != 'Class']\n",
    "y = dfFinal.iloc[:, dfFinal.columns == 'Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLPC = MLPClassifier(hidden_layer_sizes=(200,), max_iter=10000)\n",
    "MLPC.fit(X_train, y_train.values.ravel())\n",
    "y_pred = MLPC.predict(X_test)\n",
    "# Obtenemos valores de recall\n",
    "recall_acc = recall_score(y_test,y_pred)\n",
    "print(recall_acc)\n",
    "print(MLPC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soglia = 0.05\n",
    "#print(corr[\"Class\"] < soglia)\n",
    "#droppiamo V13 e V23 perchè minori della soglia\n",
    "dfFinal = df.drop([\"V13\", \"V23\"], axis = 1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "standardScaler = preprocessing.StandardScaler()\n",
    "scaledTime = standardScaler.fit_transform(X = dfFinal[['Time']])\n",
    "scaledAmount = standardScaler.fit_transform(X = dfFinal[['Amount']])\n",
    "\n",
    "dfFinal['ScaledTime'] = scaledTime\n",
    "dfFinal['ScaledAmount'] = scaledAmount\n",
    "\n",
    "#ELIMINO V26 PERCHè NELLA MATRICE DI CORRELAZIONE DEL PASSO SUCCESSIVO, V26 è MINORE DI 0.05\n",
    "dfFinal = dfFinal.drop(['Time', 'Amount', 'V26', 'ScaledTime'], axis = 1)\n",
    "\n",
    "X = dfFinal.iloc[:, dfFinal.columns != 'Class']\n",
    "y = dfFinal.iloc[:, dfFinal.columns == 'Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#hidden_layer_sizes=(200,)\n",
    "MLPC = MLPClassifier(hidden_layer_sizes=(200, 26), max_iter=10000, activation=\"tanh\", verbose=True)\n",
    "MLPC.fit(X_train, y_train.values.ravel())\n",
    "y_pred = MLPC.predict(X_test)\n",
    "# Obtenemos valores de recall\n",
    "recall_acc = recall_score(y_test, y_pred)\n",
    "print(recall_acc)\n",
    "print(MLPC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisione del dataset in training set e validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle before creating subsamples\n",
    "\n",
    "df_fraud = dfFinal.loc[dfFinal.Class == 1]\n",
    "df_non_fraud = dfFinal.loc[dfFinal.Class == 0][1000:1492]\n",
    "\n",
    "dfFifty = pd.concat([df_fraud, df_non_fraud])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = dfFifty.corr()\n",
    "corr = pd.DataFrame.abs(corr)\n",
    "fig = sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
    "fig.set_title('Correlation Matrix', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(corr[\"Class\"] <= 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione del modello di predizione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione dell'errore del modello ottenuto al passo precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "df_fraud = dfFinal.loc[dfFinal.Class == 1]\n",
    "df_non_fraud = dfFinal.loc[dfFinal.Class == 0][0:492]\n",
    "\n",
    "dfFifty = pd.concat([df_fraud, df_non_fraud])\n",
    "\n",
    "fisherX = np.array(dfFifty.iloc[:, dfFifty.columns != 'Class'])\n",
    "fisherY = np.array(dfFifty.iloc[:, dfFifty.columns == 'Class'])\n",
    "fisherY = fisherY.reshape(1, -1)[0]\n",
    "\n",
    "idx = fisher_score.fisher_score(fisherX, fisherY) #returns rank directly instead of fisher score. so no need for feature_ranking\n",
    "idx = fisher_score.feature_ranking(idx)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino le feature irrilevanti che hanno un fisher score <= 3 - V13 e V23 eliminate per la matrice di correlazione\n",
    "dfFinal = df\n",
    "for i in range(20):\n",
    "    dfFinal.append(dfFinal.loc[dfFinal.Class == 1])\n",
    "\n",
    "dfFinal = dfFinal.drop([\"V2\", \"V4\", \"V8\", \"V11\", \"V13\", \"V23\"], axis = 1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "standardScaler = preprocessing.StandardScaler()\n",
    "scaledTime = standardScaler.fit_transform(X = dfFinal[['Time']])\n",
    "scaledAmount = standardScaler.fit_transform(X = dfFinal[['Amount']])\n",
    "\n",
    "dfFinal['ScaledTime'] = scaledTime\n",
    "dfFinal['ScaledAmount'] = scaledAmount\n",
    "\n",
    "dfFinal = dfFinal.drop(['Time', 'Amount', 'ScaledTime'], axis = 1)\n",
    "\n",
    "X = dfFinal.iloc[:, dfFinal.columns != 'Class']\n",
    "y = dfFinal.iloc[:, dfFinal.columns == 'Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#hidden_layer_sizes=(200,)\n",
    "MLPC = MLPClassifier(hidden_layer_sizes=(200,6), max_iter=10000, verbose=True)\n",
    "MLPC.fit(X_train, y_train.values.ravel())\n",
    "y_pred = MLPC.predict(X_test)\n",
    "# Obtenemos valores de recall\n",
    "recall_acc = recall_score(y_test, y_pred)\n",
    "print(recall_acc)\n",
    "print(MLPC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1result = f1_score(y_test, y_pred)\n",
    "print(f1result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "features = df[df.columns[:len(df.columns)-1]].to_numpy()\n",
    "targets = df.Class.to_numpy()\n",
    "features, targets = SMOTE().fit_resample(features,targets)\n",
    "targets = targets.reshape(-1,1)\n",
    "\n",
    "target_un,count = np.unique(targets,return_counts=True)\n",
    "plt.title(\"Class Balancing\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.bar([str(i) for i in target_un],count)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(features,\n",
    "                                                 targets,\n",
    "                                                 test_size=0.3,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "print(\"Train Feature size : \",len(X_train))\n",
    "print(\"Train Label size   :\",len(Y_train))\n",
    "print(\"Test Feature size  :\",len(X_test))\n",
    "print(\"Test Label size    :\",len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, verbose=1)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train score     : {:.2f} %\".format(accuracy_score(model.predict(X_train),Y_train)*100))\n",
    "print(\"Test score      : {:.2f} %\".format(accuracy_score(model.predict(X_test),Y_test)*100))\n",
    "print(\"Precision score : {:.2f} %\".format(accuracy_score(model.predict(X_test),Y_test)*100))\n",
    "print(\"Recall score    : {:.2f} %\".format(accuracy_score(model.predict(X_test),Y_test)*100))\n",
    "\n",
    "cm = confusion_matrix(model.predict(X_test),\n",
    "                      Y_test,\n",
    "                      labels=[0,1])\n",
    "\n",
    "cmatrix = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=[\"Not Fraud\",\"Fraud\"])\n",
    "cmatrix.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17f1067490f984ba715149d06ea4a88ce507d8a2668e250361db3c636d63351c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mongoDB': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
